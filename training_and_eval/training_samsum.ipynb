{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "samsum + automin",
      "provenance": [],
      "mount_file_id": "18li1_3yKjy8fiUijVLR3JrBqHgSTzeSj",
      "authorship_tag": "ABX9TyPhG/SAPTLz2pmi8JE762cJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iftwigs/thesis-cnn-dm-automin/blob/main/training_and_eval/training_samsum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umuuoQgfbH46",
        "outputId": "74b13cde-cbe7-4d04-d86d-f59ff9f8b2fe"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 73610, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 73610 (delta 7), reused 11 (delta 0), pack-reused 73584\u001b[K\n",
            "Receiving objects: 100% (73610/73610), 56.51 MiB | 29.05 MiB/s, done.\n",
            "Resolving deltas: 100% (52350/52350), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyAj5yVYbx4d",
        "outputId": "3a99fc13-2265-435d-ae90-ad6e5a1f9fd1"
      },
      "source": [
        "!pip install datasets sacremoses sentencepiece sacrebleu bleu rouge rouge_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/f8/ff7cd6e3b400b33dcbbfd31c6c1481678a2b2f669f521ad20053009a9aa3/datasets-1.7.0-py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 5.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 17.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.0MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting bleu\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/df/4fd9bfe6dc240a1760f8e95ca41aa31382e328e31de45145f528dab5c7f8/bleu-0.3.tar.gz\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting efficiency\n",
            "  Downloading https://files.pythonhosted.org/packages/34/c9/ecc9958e1ca22b5e060a2685405e27c44aaf921e53e3318136fff5ed60e2/efficiency-0.5.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from efficiency->bleu) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (56.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.1.3)\n",
            "Building wheels for collected packages: bleu, efficiency\n",
            "  Building wheel for bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleu: filename=bleu-0.3-cp37-none-any.whl size=5802 sha256=adb87da48046adf496c8e1d96ae3edb128c35fe71a70b3f1d356d0e6927c6f18\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/95/e7/cb43a1c509c38fedbee6223963e34a51a94d8991f3b3e1888e\n",
            "  Building wheel for efficiency (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficiency: filename=efficiency-0.5-cp37-none-any.whl size=20285 sha256=31c93d766a15043fc0941f7d3d0fec80d9c66b8bf657f7ffbb649d37449ff276\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/fd/e6/2d34e80768fb2d001fc35b033a48114b1afdb6ea020b4cb2c7\n",
            "Successfully built bleu efficiency\n",
            "Installing collected packages: xxhash, fsspec, huggingface-hub, datasets, sacremoses, sentencepiece, portalocker, sacrebleu, efficiency, bleu, rouge, rouge-score\n",
            "Successfully installed bleu-0.3 datasets-1.7.0 efficiency-0.5 fsspec-2021.5.0 huggingface-hub-0.0.9 portalocker-2.0.0 rouge-1.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsVKHaIIbz4a",
        "outputId": "4bcee90b-22ef-4a72-9a8a-e66039360ced"
      },
      "source": [
        "cd transformers/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xccIQrakczHB",
        "outputId": "03803101-f9d7-4761-9551-a43b8e2e6ed1"
      },
      "source": [
        "!pip install py7zr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py7zr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/1c/d3e3a80fa8901fc232ec11ec0f2886c7e06cf38f3f40876438ada5659211/py7zr-0.16.1-py3-none-any.whl (65kB)\n",
            "\r\u001b[K     |█████                           | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.0MB/s \n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
            "  Downloading https://files.pythonhosted.org/packages/22/31/ec5f46fd4c83185b806aa9c736e228cb780f13990a9cf4da0beb70025fcc/multivolumefile-0.2.3-py3-none-any.whl\n",
            "Collecting texttable\n",
            "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/9d/99a949925b5fc9604cb65219951fd270ef30d0fd4f064d1b363eb8bb5e9b/pycryptodomex-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 15.2MB/s \n",
            "\u001b[?25hCollecting bcj-cffi<0.6.0,>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/1f/408e7b01375c863e01c25b1475d628deab7c9f85aeb74cced4caa5a512ce/bcj_cffi-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Collecting pyppmd>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/25/d8a1f334cb3b84b734b52158233f036c1dd61c8649b90c9a3cd39be9309e/pyppmd-0.15.0-cp37-cp37m-manylinux2014_x86_64.whl (121kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 50.0MB/s \n",
            "\u001b[?25hCollecting pyzstd<0.15.0,>=0.14.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/e9/fe897f8bb96163645a5b2d3a60ff8bfa6fcdedff4691a3c6c861b0324ef4/pyzstd-0.14.4-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 52.3MB/s \n",
            "\u001b[?25hCollecting brotli>=1.0.9; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ea/5bd575511b37bbd1c794606a0a621e6feff8e96b7dd007a86a5d218b2d94/Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from py7zr) (4.0.1)\n",
            "Requirement already satisfied: cffi>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from bcj-cffi<0.6.0,>=0.5.1->py7zr) (1.14.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->py7zr) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->py7zr) (3.7.4.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.0->bcj-cffi<0.6.0,>=0.5.1->py7zr) (2.20)\n",
            "Installing collected packages: multivolumefile, texttable, pycryptodomex, bcj-cffi, pyppmd, pyzstd, brotli, py7zr\n",
            "Successfully installed bcj-cffi-0.5.1 brotli-1.0.9 multivolumefile-0.2.3 py7zr-0.16.1 pycryptodomex-3.10.1 pyppmd-0.15.0 pyzstd-0.14.4 texttable-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrq_j0FRb4lh",
        "outputId": "18718eef-27b4-4752-bf5f-b7f8481c8bc8"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path t5-small \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --dataset_name samsum \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /content/drive/MyDrive/tst-summarization-samsum \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-27 20:18:53.824402: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/27/2021 20:18:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/27/2021 20:18:56 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/tst-summarization-samsum', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May27_20-18-56_4143a5f9aa01', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/tst-summarization-samsum', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "Downloading: 3.54kB [00:00, 3.52MB/s]       \n",
            "Downloading: 1.61kB [00:00, 1.86MB/s]     \n",
            "Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n",
            "Downloading: 2.94MB [00:04, 709kB/s]\n",
            "Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprmk5klaq\n",
            "Downloading: 100% 1.20k/1.20k [00:00<00:00, 959kB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphpumgdzl\n",
            "Downloading: 100% 792k/792k [00:00<00:00, 3.12MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "creating metadata file for /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppf9hhmds\n",
            "Downloading: 100% 1.39M/1.39M [00:00<00:00, 4.28MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "creating metadata file for /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdxl1ufbv\n",
            "Downloading: 100% 242M/242M [00:04<00:00, 48.7MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 15/15 [00:14<00:00,  1.06ba/s]\n",
            "100% 1/1 [00:00<00:00,  1.30ba/s]\n",
            "100% 1/1 [00:00<00:00,  1.27ba/s]\n",
            "Downloading: 5.61kB [00:00, 5.50MB/s]       \n",
            "***** Running training *****\n",
            "  Num examples = 14732\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 11049\n",
            "{'loss': 2.2088, 'learning_rate': 4.7737351796542676e-05, 'epoch': 0.14}\n",
            "  5% 500/11049 [00:32<10:45, 16.35it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-500/spiece.model\n",
            "{'loss': 2.0754, 'learning_rate': 4.547470359308535e-05, 'epoch': 0.27}\n",
            "  9% 1000/11049 [01:10<10:14, 16.37it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1000/spiece.model\n",
            "{'loss': 2.0187, 'learning_rate': 4.321205538962802e-05, 'epoch': 0.41}\n",
            " 14% 1500/11049 [01:47<10:35, 15.02it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-1500/spiece.model\n",
            "{'loss': 2.0091, 'learning_rate': 4.09494071861707e-05, 'epoch': 0.54}\n",
            " 18% 2000/11049 [02:26<10:00, 15.06it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2000/spiece.model\n",
            "{'loss': 1.9721, 'learning_rate': 3.868675898271337e-05, 'epoch': 0.68}\n",
            " 23% 2500/11049 [03:04<09:20, 15.25it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-2500/spiece.model\n",
            "{'loss': 1.958, 'learning_rate': 3.6424110779256044e-05, 'epoch': 0.81}\n",
            " 27% 3000/11049 [03:43<08:40, 15.46it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3000/spiece.model\n",
            "{'loss': 1.942, 'learning_rate': 3.416146257579871e-05, 'epoch': 0.95}\n",
            " 32% 3500/11049 [04:21<08:43, 14.41it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-3500/spiece.model\n",
            "{'loss': 1.8903, 'learning_rate': 3.189881437234139e-05, 'epoch': 1.09}\n",
            " 36% 4000/11049 [05:00<07:12, 16.31it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4000/spiece.model\n",
            "{'loss': 1.8889, 'learning_rate': 2.963616616888406e-05, 'epoch': 1.22}\n",
            " 41% 4500/11049 [05:39<07:01, 15.52it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-4500/spiece.model\n",
            "{'loss': 1.8651, 'learning_rate': 2.7373517965426738e-05, 'epoch': 1.36}\n",
            " 45% 5000/11049 [06:18<06:00, 16.77it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5000/spiece.model\n",
            "{'loss': 1.8896, 'learning_rate': 2.511086976196941e-05, 'epoch': 1.49}\n",
            " 50% 5500/11049 [06:57<06:07, 15.10it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-5500/spiece.model\n",
            "{'loss': 1.8663, 'learning_rate': 2.2848221558512085e-05, 'epoch': 1.63}\n",
            " 54% 6000/11049 [07:36<05:29, 15.32it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6000/spiece.model\n",
            "{'loss': 1.8473, 'learning_rate': 2.058557335505476e-05, 'epoch': 1.76}\n",
            " 59% 6500/11049 [08:18<05:11, 14.60it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-6500/spiece.model\n",
            "{'loss': 1.8097, 'learning_rate': 1.8322925151597432e-05, 'epoch': 1.9}\n",
            " 63% 7000/11049 [09:00<04:47, 14.09it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7000/spiece.model\n",
            "{'loss': 1.8223, 'learning_rate': 1.6060276948140106e-05, 'epoch': 2.04}\n",
            " 68% 7500/11049 [09:42<04:43, 12.52it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-7500/spiece.model\n",
            "{'loss': 1.8113, 'learning_rate': 1.3797628744682778e-05, 'epoch': 2.17}\n",
            " 72% 8000/11049 [10:24<03:25, 14.83it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8000/spiece.model\n",
            "{'loss': 1.798, 'learning_rate': 1.1534980541225452e-05, 'epoch': 2.31}\n",
            " 77% 8500/11049 [11:07<03:11, 13.28it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-8500/spiece.model\n",
            "{'loss': 1.7957, 'learning_rate': 9.272332337768125e-06, 'epoch': 2.44}\n",
            " 81% 9000/11049 [11:50<02:31, 13.49it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9000/spiece.model\n",
            "{'loss': 1.8204, 'learning_rate': 7.009684134310799e-06, 'epoch': 2.58}\n",
            " 86% 9500/11049 [12:31<01:48, 14.28it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-9500/spiece.model\n",
            "{'loss': 1.8147, 'learning_rate': 4.747035930853471e-06, 'epoch': 2.72}\n",
            " 91% 10000/11049 [13:10<01:09, 15.10it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10000/spiece.model\n",
            "{'loss': 1.8154, 'learning_rate': 2.4843877273961445e-06, 'epoch': 2.85}\n",
            " 95% 10500/11049 [13:49<00:34, 15.84it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-10500/spiece.model\n",
            "{'loss': 1.8219, 'learning_rate': 2.21739523938818e-07, 'epoch': 2.99}\n",
            "100% 11000/11049 [14:29<00:03, 14.72it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-11000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-11000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/checkpoint-11000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/checkpoint-11000/spiece.model\n",
            "100% 11048/11049 [14:36<00:00, 13.96it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 876.17, 'train_samples_per_second': 50.442, 'train_steps_per_second': 12.611, 'epoch': 3.0}\n",
            "100% 11049/11049 [14:36<00:00, 12.61it/s]\n",
            "Saving model checkpoint to /content/drive/MyDrive/tst-summarization-samsum\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-samsum/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-samsum/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-samsum/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-samsum/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_runtime            = 0:14:36.16\n",
            "  train_samples            =      14732\n",
            "  train_samples_per_second =     50.442\n",
            "  train_steps_per_second   =     12.611\n",
            "05/27/2021 20:34:16 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 818\n",
            "  Batch size = 4\n",
            "100% 205/205 [02:08<00:00,  1.60it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_gen_len            =    22.5697\n",
            "  eval_loss               =     1.6919\n",
            "  eval_rouge1             =    44.9338\n",
            "  eval_rouge2             =    21.5456\n",
            "  eval_rougeL             =    37.7076\n",
            "  eval_rougeLsum          =     41.672\n",
            "  eval_runtime            = 0:02:08.67\n",
            "  eval_samples            =        818\n",
            "  eval_samples_per_second =      6.357\n",
            "  eval_steps_per_second   =      1.593\n",
            "05/27/2021 20:36:25 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 819\n",
            "  Batch size = 4\n",
            "100% 205/205 [00:59<00:00,  3.53it/s]***** predict metrics *****\n",
            "  predict_gen_len            =    22.3993\n",
            "  predict_loss               =     1.7133\n",
            "  predict_rouge1             =    44.8483\n",
            "  predict_rouge2             =     20.374\n",
            "  predict_rougeL             =     37.124\n",
            "  predict_rougeLsum          =    41.0815\n",
            "  predict_runtime            = 0:02:04.18\n",
            "  predict_samples            =        819\n",
            "  predict_samples_per_second =      6.595\n",
            "  predict_steps_per_second   =      1.651\n",
            "100% 205/205 [02:34<00:00,  1.32it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T91zEYOTcmRN",
        "outputId": "272718b6-cba5-4658-f3d2-aa3241dee8ed"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization-samsum \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --train_file /content/drive/MyDrive/automin_processed/train_automin_x4.csv \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin_x4.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin_x4.csv \\\n",
        "    --output_dir /content/drive/MyDrive/t5-base-automin-samsum-x4 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --num_train_epochs=10 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-28 21:42:47.837693: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/28/2021 21:42:51 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/28/2021 21:42:51 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/t5-base-automin-samsum-x4', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May28_21-42-51_98e05fff5bd1', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/t5-base-automin-samsum-x4', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/28/2021 21:42:52 - WARNING - datasets.builder -   Using custom data configuration default-480ff060fe9bd0a4\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-480ff060fe9bd0a4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-480ff060fe9bd0a4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-samsum/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-samsum/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization-samsum/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization-samsum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [01:24<00:00, 84.72s/ba]\n",
            "100% 1/1 [00:07<00:00,  7.64s/ba]\n",
            "100% 1/1 [00:13<00:00, 13.82s/ba]\n",
            "Downloading: 5.61kB [00:00, 3.58MB/s]       \n",
            "***** Running training *****\n",
            "  Num examples = 340\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 850\n",
            "{'loss': 3.4005, 'learning_rate': 2.058823529411765e-05, 'epoch': 5.88}\n",
            " 59% 500/850 [01:28<01:01,  5.70it/s]Saving model checkpoint to /content/drive/MyDrive/t5-base-automin-samsum-x4/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/checkpoint-500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/t5-base-automin-samsum-x4/checkpoint-500/spiece.model\n",
            "100% 850/850 [02:35<00:00,  5.68it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 155.1978, 'train_samples_per_second': 21.908, 'train_steps_per_second': 5.477, 'epoch': 10.0}\n",
            "100% 850/850 [02:35<00:00,  5.48it/s]\n",
            "Saving model checkpoint to /content/drive/MyDrive/t5-base-automin-samsum-x4\n",
            "Configuration saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/config.json\n",
            "Model weights saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/t5-base-automin-samsum-x4/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/t5-base-automin-samsum-x4/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_runtime            = 0:02:35.19\n",
            "  train_samples            =        340\n",
            "  train_samples_per_second =     21.908\n",
            "  train_steps_per_second   =      5.477\n",
            "05/28/2021 21:47:43 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 40\n",
            "  Batch size = 4\n",
            "100% 10/10 [00:16<00:00,  1.66s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_gen_len            =      126.1\n",
            "  eval_loss               =     3.0889\n",
            "  eval_rouge1             =    10.8757\n",
            "  eval_rouge2             =     0.9705\n",
            "  eval_rougeL             =     8.3124\n",
            "  eval_rougeLsum          =     9.1934\n",
            "  eval_runtime            = 0:00:18.03\n",
            "  eval_samples            =         40\n",
            "  eval_samples_per_second =      2.218\n",
            "  eval_steps_per_second   =      0.555\n",
            "05/28/2021 21:48:01 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 72\n",
            "  Batch size = 4\n",
            "100% 18/18 [00:19<00:00,  1.11s/it]***** predict metrics *****\n",
            "  predict_gen_len            =   124.7778\n",
            "  predict_loss               =     3.4963\n",
            "  predict_rouge1             =    10.3014\n",
            "  predict_rouge2             =     1.0395\n",
            "  predict_rougeL             =     7.4565\n",
            "  predict_rougeLsum          =     8.3036\n",
            "  predict_runtime            = 0:00:30.05\n",
            "  predict_samples            =         72\n",
            "  predict_samples_per_second =      2.396\n",
            "  predict_steps_per_second   =      0.599\n",
            "100% 18/18 [00:33<00:00,  1.87s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC1PaobTjbVP",
        "outputId": "75947be6-0059-4d92-ba4c-318933f2dd12"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path t5-small \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --dataset_name xsum \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --max_train_samples=25500 \\\n",
        "    --output_dir /content/drive/MyDrive/tst-summarization-xsum \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-27 20:55:37.110281: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/27/2021 20:55:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/27/2021 20:55:38 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/tst-summarization-xsum', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May27_20-55-38_4143a5f9aa01', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/tst-summarization-xsum', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/27/2021 20:55:39 - WARNING - datasets.builder -   Using custom data configuration default\n",
            "05/27/2021 20:55:39 - WARNING - datasets.builder -   Reusing dataset xsum (/root/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 26/26 [00:53<00:00,  2.07s/ba]\n",
            "100% 12/12 [00:22<00:00,  1.88s/ba]\n",
            "100% 12/12 [00:23<00:00,  1.95s/ba]\n",
            "***** Running training *****\n",
            "  Num examples = 25500\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 19125\n",
            "{'loss': 3.02, 'learning_rate': 4.869281045751634e-05, 'epoch': 0.08}\n",
            "  3% 500/19125 [01:12<46:47,  6.63it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-500/spiece.model\n",
            "{'loss': 2.8605, 'learning_rate': 4.738562091503268e-05, 'epoch': 0.16}\n",
            "  5% 1000/19125 [02:32<50:43,  5.96it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1000/spiece.model\n",
            "{'loss': 2.8355, 'learning_rate': 4.607843137254902e-05, 'epoch': 0.24}\n",
            "  8% 1500/19125 [03:50<47:47,  6.15it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-1500/spiece.model\n",
            "{'loss': 2.8155, 'learning_rate': 4.477124183006536e-05, 'epoch': 0.31}\n",
            " 10% 2000/19125 [05:08<45:21,  6.29it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2000/spiece.model\n",
            "{'loss': 2.7703, 'learning_rate': 4.3464052287581704e-05, 'epoch': 0.39}\n",
            " 13% 2500/19125 [06:24<43:56,  6.31it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-2500/spiece.model\n",
            "{'loss': 2.8005, 'learning_rate': 4.215686274509804e-05, 'epoch': 0.47}\n",
            " 16% 3000/19125 [07:40<40:04,  6.71it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3000/spiece.model\n",
            "{'loss': 2.7756, 'learning_rate': 4.084967320261438e-05, 'epoch': 0.55}\n",
            " 18% 3500/19125 [08:58<39:26,  6.60it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-3500/spiece.model\n",
            "{'loss': 2.7661, 'learning_rate': 3.954248366013072e-05, 'epoch': 0.63}\n",
            " 21% 4000/19125 [10:16<35:11,  7.16it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4000/spiece.model\n",
            "{'loss': 2.7604, 'learning_rate': 3.8235294117647055e-05, 'epoch': 0.71}\n",
            " 24% 4500/19125 [11:32<34:40,  7.03it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-4500/spiece.model\n",
            "{'loss': 2.7585, 'learning_rate': 3.6928104575163405e-05, 'epoch': 0.78}\n",
            " 26% 5000/19125 [12:46<31:31,  7.47it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5000/spiece.model\n",
            "{'loss': 2.7543, 'learning_rate': 3.562091503267974e-05, 'epoch': 0.86}\n",
            " 29% 5500/19125 [14:01<32:40,  6.95it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-5500/spiece.model\n",
            "{'loss': 2.7434, 'learning_rate': 3.431372549019608e-05, 'epoch': 0.94}\n",
            " 31% 6000/19125 [15:16<29:41,  7.37it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6000/spiece.model\n",
            "{'loss': 2.6997, 'learning_rate': 3.300653594771242e-05, 'epoch': 1.02}\n",
            " 34% 6500/19125 [16:31<32:21,  6.50it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-6500/spiece.model\n",
            "{'loss': 2.666, 'learning_rate': 3.169934640522876e-05, 'epoch': 1.1}\n",
            " 37% 7000/19125 [17:44<23:27,  8.62it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7000/spiece.model\n",
            "{'loss': 2.6585, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.18}\n",
            " 39% 7500/19125 [19:00<27:46,  6.98it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-7500/spiece.model\n",
            "{'loss': 2.6632, 'learning_rate': 2.9084967320261443e-05, 'epoch': 1.25}\n",
            " 42% 8000/19125 [20:16<27:52,  6.65it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8000/spiece.model\n",
            "{'loss': 2.6557, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n",
            " 44% 8500/19125 [21:32<21:40,  8.17it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-8500/spiece.model\n",
            "{'loss': 2.6532, 'learning_rate': 2.647058823529412e-05, 'epoch': 1.41}\n",
            " 47% 9000/19125 [22:48<23:06,  7.30it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9000/spiece.model\n",
            "{'loss': 2.6565, 'learning_rate': 2.516339869281046e-05, 'epoch': 1.49}\n",
            " 50% 9500/19125 [24:04<23:22,  6.86it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-9500/spiece.model\n",
            "{'loss': 2.6492, 'learning_rate': 2.38562091503268e-05, 'epoch': 1.57}\n",
            " 52% 10000/19125 [25:18<24:30,  6.20it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10000/spiece.model\n",
            "{'loss': 2.6402, 'learning_rate': 2.2549019607843138e-05, 'epoch': 1.65}\n",
            " 55% 10500/19125 [26:34<19:33,  7.35it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-10500/spiece.model\n",
            "{'loss': 2.6486, 'learning_rate': 2.1241830065359477e-05, 'epoch': 1.73}\n",
            " 58% 11000/19125 [27:49<21:32,  6.29it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11000/spiece.model\n",
            "{'loss': 2.6489, 'learning_rate': 1.993464052287582e-05, 'epoch': 1.8}\n",
            " 60% 11500/19125 [29:04<19:03,  6.67it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-11500/spiece.model\n",
            "{'loss': 2.6549, 'learning_rate': 1.862745098039216e-05, 'epoch': 1.88}\n",
            " 63% 12000/19125 [30:19<17:34,  6.75it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12000/spiece.model\n",
            "{'loss': 2.6352, 'learning_rate': 1.7320261437908496e-05, 'epoch': 1.96}\n",
            " 65% 12500/19125 [31:35<15:32,  7.10it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-12500/spiece.model\n",
            "{'loss': 2.6273, 'learning_rate': 1.6013071895424836e-05, 'epoch': 2.04}\n",
            " 68% 13000/19125 [32:51<13:23,  7.63it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13000/spiece.model\n",
            "{'loss': 2.604, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.12}\n",
            " 71% 13500/19125 [34:05<14:09,  6.62it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-13500/spiece.model\n",
            "{'loss': 2.6278, 'learning_rate': 1.3398692810457516e-05, 'epoch': 2.2}\n",
            " 73% 14000/19125 [35:22<13:35,  6.29it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14000/spiece.model\n",
            "{'loss': 2.6245, 'learning_rate': 1.2091503267973856e-05, 'epoch': 2.27}\n",
            " 76% 14500/19125 [36:38<11:18,  6.82it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-14500/spiece.model\n",
            "{'loss': 2.6126, 'learning_rate': 1.0784313725490197e-05, 'epoch': 2.35}\n",
            " 78% 15000/19125 [37:54<09:58,  6.89it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15000/spiece.model\n",
            "{'loss': 2.5855, 'learning_rate': 9.477124183006535e-06, 'epoch': 2.43}\n",
            " 81% 15500/19125 [39:10<08:28,  7.13it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-15500/spiece.model\n",
            "{'loss': 2.5989, 'learning_rate': 8.169934640522877e-06, 'epoch': 2.51}\n",
            " 84% 16000/19125 [40:27<08:40,  6.01it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16000/spiece.model\n",
            "{'loss': 2.5859, 'learning_rate': 6.862745098039216e-06, 'epoch': 2.59}\n",
            " 86% 16500/19125 [41:44<06:20,  6.90it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-16500/spiece.model\n",
            "{'loss': 2.5836, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n",
            " 89% 17000/19125 [43:01<05:16,  6.72it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17000/spiece.model\n",
            "{'loss': 2.5998, 'learning_rate': 4.2483660130718954e-06, 'epoch': 2.75}\n",
            " 92% 17500/19125 [44:19<03:24,  7.95it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-17500/spiece.model\n",
            "{'loss': 2.5942, 'learning_rate': 2.9411764705882355e-06, 'epoch': 2.82}\n",
            " 94% 18000/19125 [45:33<02:39,  7.04it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18000/spiece.model\n",
            "{'loss': 2.587, 'learning_rate': 1.6339869281045753e-06, 'epoch': 2.9}\n",
            " 97% 18500/19125 [46:50<01:26,  7.20it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-18500/spiece.model\n",
            "{'loss': 2.6, 'learning_rate': 3.2679738562091505e-07, 'epoch': 2.98}\n",
            " 99% 19000/19125 [48:04<00:18,  6.90it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-19000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-19000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/checkpoint-19000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/checkpoint-19000/spiece.model\n",
            "100% 19125/19125 [48:27<00:00,  7.36it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2907.3282, 'train_samples_per_second': 26.313, 'train_steps_per_second': 6.578, 'epoch': 3.0}\n",
            "100% 19125/19125 [48:27<00:00,  6.58it/s]\n",
            "Saving model checkpoint to /content/drive/MyDrive/tst-summarization-xsum\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-xsum/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-xsum/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-xsum/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-xsum/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-xsum/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_runtime            = 0:48:27.32\n",
            "  train_samples            =      25500\n",
            "  train_samples_per_second =     26.313\n",
            "  train_steps_per_second   =      6.578\n",
            "05/27/2021 21:45:55 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 11332\n",
            "  Batch size = 4\n",
            "100% 2833/2833 [33:29<00:00,  1.41it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_gen_len            =    26.9609\n",
            "  eval_loss               =     2.4392\n",
            "  eval_rouge1             =    30.4699\n",
            "  eval_rouge2             =     8.7226\n",
            "  eval_rougeL             =    23.4943\n",
            "  eval_rougeLsum          =    23.4947\n",
            "  eval_runtime            = 0:33:30.26\n",
            "  eval_samples            =      11332\n",
            "  eval_samples_per_second =      5.637\n",
            "  eval_steps_per_second   =      1.409\n",
            "05/27/2021 22:19:26 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 11334\n",
            "  Batch size = 4\n",
            "100% 2834/2834 [17:41<00:00,  2.88it/s]***** predict metrics *****\n",
            "  predict_gen_len            =    26.8196\n",
            "  predict_loss               =     2.4564\n",
            "  predict_rouge1             =    30.4839\n",
            "  predict_rouge2             =     8.6632\n",
            "  predict_rougeL             =    23.4109\n",
            "  predict_rougeLsum          =    23.4088\n",
            "  predict_runtime            = 0:32:46.94\n",
            "  predict_samples            =      11334\n",
            "  predict_samples_per_second =      5.762\n",
            "  predict_steps_per_second   =      1.441\n",
            "100% 2834/2834 [39:57<00:00,  1.18it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5l38sIkkSn8",
        "outputId": "fed6cb75-4053-4b62-99b4-a32216b0e452"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization-xsum \\\n",
        "    --do_predict \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin.csv \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /content/drive/MyDrive/eval-xsum-automin \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-28 11:51:37.864135: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/28/2021 11:51:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/28/2021 11:51:40 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/eval-xsum-automin', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May28_11-51-40_ce1584032255', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/eval-xsum-automin', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/28/2021 11:51:40 - WARNING - datasets.builder -   Using custom data configuration default-8f3b29795c3b552d\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-8f3b29795c3b552d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8f3b29795c3b552d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-xsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-xsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-xsum/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-xsum/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization-xsum/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization-xsum/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization-xsum/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization-xsum/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization-xsum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:03<00:00,  3.33s/ba]\n",
            "Downloading: 5.61kB [00:00, 5.54MB/s]       \n",
            "05/28/2021 11:52:01 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 18\n",
            "  Batch size = 4\n",
            "100% 5/5 [00:02<00:00,  1.92it/s]***** predict metrics *****\n",
            "  predict_gen_len            =       49.0\n",
            "  predict_loss               =      5.183\n",
            "  predict_rouge1             =     6.6822\n",
            "  predict_rouge2             =     0.7404\n",
            "  predict_rougeL             =     5.7448\n",
            "  predict_rougeLsum          =     6.3773\n",
            "  predict_runtime            = 0:00:06.05\n",
            "  predict_samples            =         18\n",
            "  predict_samples_per_second =      2.972\n",
            "  predict_steps_per_second   =      0.826\n",
            "100% 5/5 [00:05<00:00,  1.14s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wtQB6MD0LOq",
        "outputId": "ecefaae2-8da1-429f-d6c4-a0100a95eef4"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization-xsum \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --train_file /content/drive/MyDrive/automin_processed/train_automin.csv \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin.csv \\\n",
        "    --output_dir /content/drive/MyDrive/xsum-automin \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --num_train_epochs=10 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-28 11:55:47.043157: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/28/2021 11:55:48 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/28/2021 11:55:48 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/xsum-automin', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May28_11-55-48_ce1584032255', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/xsum-automin', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/28/2021 11:55:48 - WARNING - datasets.builder -   Using custom data configuration default-8f2b4e3cae78ab30\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-8f2b4e3cae78ab30/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8f2b4e3cae78ab30/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-xsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-xsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-xsum/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-xsum/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization-xsum/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization-xsum/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization-xsum/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization-xsum/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization-xsum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:20<00:00, 20.61s/ba]\n",
            "100% 1/1 [00:01<00:00,  1.83s/ba]\n",
            "100% 1/1 [00:03<00:00,  3.27s/ba]\n",
            "***** Running training *****\n",
            "  Num examples = 85\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 220\n",
            "100% 220/220 [00:57<00:00,  4.40it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 58.004, 'train_samples_per_second': 14.654, 'train_steps_per_second': 3.793, 'epoch': 10.0}\n",
            "100% 220/220 [00:57<00:00,  3.79it/s]\n",
            "Saving model checkpoint to /content/drive/MyDrive/xsum-automin\n",
            "Configuration saved in /content/drive/MyDrive/xsum-automin/config.json\n",
            "Model weights saved in /content/drive/MyDrive/xsum-automin/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/xsum-automin/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/xsum-automin/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/xsum-automin/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_runtime            = 0:00:58.00\n",
            "  train_samples            =         85\n",
            "  train_samples_per_second =     14.654\n",
            "  train_steps_per_second   =      3.793\n",
            "05/28/2021 11:57:18 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 4\n",
            "100% 3/3 [00:03<00:00,  1.20s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_gen_len            =      127.0\n",
            "  eval_loss               =     3.3535\n",
            "  eval_rouge1             =     8.6779\n",
            "  eval_rouge2             =     0.4907\n",
            "  eval_rougeL             =     7.5716\n",
            "  eval_rougeLsum          =     7.2876\n",
            "  eval_runtime            = 0:00:04.70\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      2.123\n",
            "  eval_steps_per_second   =      0.637\n",
            "05/28/2021 11:57:23 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 18\n",
            "  Batch size = 4\n",
            "100% 5/5 [00:04<00:00,  1.04it/s]***** predict metrics *****\n",
            "  predict_gen_len            =   113.3889\n",
            "  predict_loss               =     3.6773\n",
            "  predict_rouge1             =     7.8434\n",
            "  predict_rouge2             =     0.4346\n",
            "  predict_rougeL             =     6.4079\n",
            "  predict_rougeLsum          =     6.2624\n",
            "  predict_runtime            = 0:00:08.66\n",
            "  predict_samples            =         18\n",
            "  predict_samples_per_second =      2.077\n",
            "  predict_steps_per_second   =      0.577\n",
            "100% 5/5 [00:08<00:00,  1.69s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vb78dCZ1Iyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d04ab3-b177-4171-c1ec-f14f9ffb10dd"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization-samsum \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --train_file /content/drive/MyDrive/automin_processed/train_automin_clean_monge.csv \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin_x4.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin_x4.csv \\\n",
        "    --output_dir /content/drive/MyDrive/t5-base-automin-samsum-clean \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --num_train_epochs=10 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-29 10:33:53.092194: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/29/2021 10:33:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/29/2021 10:33:54 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/t5-base-automin-samsum-clean', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May29_10-33-54_a7173e9e2689', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/t5-base-automin-samsum-clean', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/29/2021 10:33:54 - WARNING - datasets.builder -   Using custom data configuration default-480ff060fe9bd0a4\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-480ff060fe9bd0a4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-480ff060fe9bd0a4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-samsum/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-samsum/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization-samsum/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization-samsum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [01:27<00:00, 87.10s/ba]\n",
            "100% 1/1 [00:07<00:00,  7.57s/ba]\n",
            "  0% 0/1 [00:00<?, ?ba/s]Traceback (most recent call last):\n",
            "  File \"examples/pytorch/summarization/run_summarization.py\", line 606, in <module>\n",
            "  File \"examples/pytorch/summarization/run_summarization.py\", line 465, in main\n",
            "    load_from_cache_file=not data_args.overwrite_cache,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 1606, in map\n",
            "    desc=desc,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 176, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py\", line 397, in wrapper\n",
            "    out = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 1934, in _map_single\n",
            "    offset=offset,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\", line 1826, in apply_function_on_filtered_inputs\n",
            "    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\n",
            "  File \"examples/pytorch/summarization/run_summarization.py\", line 408, in preprocess_function\n",
            "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2305, in __call__\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2490, in batch_encode_plus\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 563, in _batch_encode_plus\n",
            "    verbose=verbose,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 613, in _batch_prepare_for_model\n",
            "    verbose=verbose,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2798, in prepare_for_model\n",
            "    stride=stride,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2903, in truncate_sequences\n",
            "    ids = ids[:-1]\n",
            "KeyboardInterrupt\n",
            "  0% 0/1 [00:11<?, ?ba/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LejbCOG9ppeZ",
        "outputId": "72cd18f3-247d-48ab-d2cb-3c50afbd2562"
      },
      "source": [
        "cd transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Y4CU5FrkIU",
        "outputId": "0af275da-d594-4a08-e7d9-566db51f46a8"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization-samsum \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin_clean.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin_clean.csv \\\n",
        "    --output_dir /content/drive/MyDrive/t5-base-automin-samsum-clean \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-29 10:40:44.345389: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/29/2021 10:40:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/29/2021 10:40:45 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/t5-base-automin-samsum-clean', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May29_10-40-45_a7173e9e2689', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/t5-base-automin-samsum-clean', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/29/2021 10:40:45 - WARNING - datasets.builder -   Using custom data configuration default-3934c0f4565bd6cb\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3934c0f4565bd6cb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3934c0f4565bd6cb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-samsum/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-samsum/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-samsum/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization-samsum/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization-samsum/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization-samsum.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00,  1.57ba/s]\n",
            "100% 1/1 [00:01<00:00,  1.74s/ba]\n",
            "05/29/2021 10:40:52 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 4\n",
            "100% 3/3 [00:02<00:00,  1.00it/s]\n",
            "***** eval metrics *****\n",
            "  eval_gen_len            =       84.4\n",
            "  eval_loss               =     5.9511\n",
            "  eval_rouge1             =     7.0609\n",
            "  eval_rouge2             =       0.32\n",
            "  eval_rougeL             =     5.1009\n",
            "  eval_rougeLsum          =     6.5477\n",
            "  eval_runtime            = 0:00:04.04\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      2.473\n",
            "  eval_steps_per_second   =      0.742\n",
            "05/29/2021 10:40:56 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 18\n",
            "  Batch size = 4\n",
            "100% 5/5 [00:03<00:00,  1.26it/s]***** predict metrics *****\n",
            "  predict_gen_len            =       71.5\n",
            "  predict_loss               =     6.4989\n",
            "  predict_rouge1             =     7.0075\n",
            "  predict_rouge2             =     0.0823\n",
            "  predict_rougeL             =     5.2414\n",
            "  predict_rougeLsum          =     6.2778\n",
            "  predict_runtime            = 0:00:07.12\n",
            "  predict_samples            =         18\n",
            "  predict_samples_per_second =      2.527\n",
            "  predict_steps_per_second   =      0.702\n",
            "100% 5/5 [00:07<00:00,  1.41s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBgJl0KxtKiE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}