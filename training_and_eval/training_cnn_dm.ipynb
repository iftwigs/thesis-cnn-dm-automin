{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thesis_baseline",
      "provenance": [],
      "mount_file_id": "1DURv9_ZNXT-R6DM9nD1P2sy2adCrZdPb",
      "authorship_tag": "ABX9TyPfDcY3b1DL6QipIQOxSTfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iftwigs/thesis-cnn-dm-automin/blob/main/training_and_eval/training_cnn_dm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIVjtG4pp_c2",
        "outputId": "9f1b1176-1e12-40c0-acf6-c4c101839895"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAOlS2KSmWX_",
        "outputId": "4e190b13-690c-48a5-9456-2e5ad0235cc1"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72857, done.\u001b[K\n",
            "remote: Counting objects: 100% (888/888), done.\u001b[K\n",
            "remote: Compressing objects: 100% (523/523), done.\u001b[K\n",
            "remote: Total 72857 (delta 507), reused 571 (delta 319), pack-reused 71969\u001b[K\n",
            "Receiving objects: 100% (72857/72857), 56.45 MiB | 21.51 MiB/s, done.\n",
            "Resolving deltas: 100% (51632/51632), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtLVVOBkmpTZ",
        "outputId": "fd41341c-9b4f-4b1e-a608-a4f5b6f4035c"
      },
      "source": [
        "!pip install datasets sacremoses sentencepiece sacrebleu bleu rouge rouge_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 14.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 26.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 41.5MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hCollecting bleu\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/df/4fd9bfe6dc240a1760f8e95ca41aa31382e328e31de45145f528dab5c7f8/bleu-0.3.tar.gz\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.9MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (8.0.0)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting efficiency\n",
            "  Downloading https://files.pythonhosted.org/packages/34/c9/ecc9958e1ca22b5e060a2685405e27c44aaf921e53e3318136fff5ed60e2/efficiency-0.5.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from efficiency->bleu) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (56.1.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.5)\n",
            "Building wheels for collected packages: bleu, efficiency\n",
            "  Building wheel for bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleu: filename=bleu-0.3-cp37-none-any.whl size=5802 sha256=5b4d6710d090efc4eab31c642b1ac8d8db173a6f18b7d4c1968274591914a212\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/95/e7/cb43a1c509c38fedbee6223963e34a51a94d8991f3b3e1888e\n",
            "  Building wheel for efficiency (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficiency: filename=efficiency-0.5-cp37-none-any.whl size=20285 sha256=aea317239b72f840ffcc8eb858d430a1483202c26aee78eee8d9d2acb8c0b2a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/fd/e6/2d34e80768fb2d001fc35b033a48114b1afdb6ea020b4cb2c7\n",
            "Successfully built bleu efficiency\n",
            "Installing collected packages: xxhash, huggingface-hub, fsspec, datasets, sacremoses, sentencepiece, portalocker, sacrebleu, efficiency, bleu, rouge, rouge-score\n",
            "Successfully installed bleu-0.3 datasets-1.6.2 efficiency-0.5 fsspec-2021.5.0 huggingface-hub-0.0.8 portalocker-2.0.0 rouge-1.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvXJt6tOI9RY",
        "outputId": "cbd8d479-a39a-47de-c9b1-3348fc1cfb36"
      },
      "source": [
        "cd transformers/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByIIafqYIPqb",
        "outputId": "c0c7f0b3-6ce0-49ca-f5da-3aec5c218a92"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path t5-small \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --dataset_name cnn_dailymail \\\n",
        "    --dataset_config \"3.0.0\" \\\n",
        "    --learning_rate=0.001 \\\n",
        "    --max_train_samples=25500 \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /content/drive/MyDrive/tst-summarization-2 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-16 16:37:16.491524: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/16/2021 16:37:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/16/2021 16:37:18 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/tst-summarization-2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May16_16-37-18_b6a649bf66a7', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/tst-summarization-2', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/16/2021 16:37:18 - WARNING - datasets.builder -   Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
            "https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe6e8rlxe\n",
            "Downloading: 100% 1.20k/1.20k [00:00<00:00, 1.22MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzd6ha4ju\n",
            "Downloading: 100% 792k/792k [00:00<00:00, 7.32MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "creating metadata file for /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk5fye8qa\n",
            "Downloading: 100% 1.39M/1.39M [00:00<00:00, 11.2MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "creating metadata file for /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpti9kax0f\n",
            "Downloading: 100% 242M/242M [00:07<00:00, 33.6MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 26/26 [01:38<00:00,  3.78s/ba]\n",
            "100% 14/14 [00:53<00:00,  3.85s/ba]\n",
            "100% 12/12 [00:46<00:00,  3.88s/ba]\n",
            "***** Running training *****\n",
            "  Num examples = 25500\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 19125\n",
            "{'loss': 2.2826, 'learning_rate': 0.0009738562091503268, 'epoch': 0.08}\n",
            "  3% 500/19125 [02:41<1:42:39,  3.02it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-500/spiece.model\n",
            "{'loss': 2.2978, 'learning_rate': 0.0009477124183006536, 'epoch': 0.16}\n",
            "  5% 1000/19125 [05:26<1:37:44,  3.09it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-1000/spiece.model\n",
            "{'loss': 2.2734, 'learning_rate': 0.0009215686274509804, 'epoch': 0.24}\n",
            "  8% 1500/19125 [08:14<1:37:17,  3.02it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-1500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-1500/spiece.model\n",
            "{'loss': 2.3039, 'learning_rate': 0.0008954248366013072, 'epoch': 0.31}\n",
            " 10% 2000/19125 [11:00<1:26:01,  3.32it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-2000/spiece.model\n",
            "{'loss': 2.2828, 'learning_rate': 0.000869281045751634, 'epoch': 0.39}\n",
            " 13% 2500/19125 [13:46<1:23:43,  3.31it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-2500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-2500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-2500/spiece.model\n",
            "{'loss': 2.2476, 'learning_rate': 0.0008431372549019609, 'epoch': 0.47}\n",
            " 16% 3000/19125 [16:34<1:28:03,  3.05it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-3000/spiece.model\n",
            "{'loss': 2.2141, 'learning_rate': 0.0008169934640522876, 'epoch': 0.55}\n",
            " 18% 3500/19125 [19:19<1:24:47,  3.07it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-3500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-3500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-3500/spiece.model\n",
            "{'loss': 2.2321, 'learning_rate': 0.0007908496732026144, 'epoch': 0.63}\n",
            " 21% 4000/19125 [22:04<1:14:57,  3.36it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-4000/spiece.model\n",
            "{'loss': 2.2184, 'learning_rate': 0.0007647058823529411, 'epoch': 0.71}\n",
            " 24% 4500/19125 [24:49<1:18:06,  3.12it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-4500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-4500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-4500/spiece.model\n",
            "{'loss': 2.2077, 'learning_rate': 0.0007385620915032681, 'epoch': 0.78}\n",
            " 26% 5000/19125 [27:34<1:12:18,  3.26it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-5000/spiece.model\n",
            "{'loss': 2.1892, 'learning_rate': 0.0007124183006535948, 'epoch': 0.86}\n",
            " 29% 5500/19125 [30:19<1:09:24,  3.27it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-5500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-5500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-5500/spiece.model\n",
            "{'loss': 2.2063, 'learning_rate': 0.0006862745098039216, 'epoch': 0.94}\n",
            " 31% 6000/19125 [33:05<1:10:11,  3.12it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-6000/spiece.model\n",
            "{'loss': 2.1208, 'learning_rate': 0.0006601307189542483, 'epoch': 1.02}\n",
            " 34% 6500/19125 [35:50<1:08:24,  3.08it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-6500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-6500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-6500/spiece.model\n",
            "{'loss': 2.0126, 'learning_rate': 0.0006339869281045752, 'epoch': 1.1}\n",
            " 37% 7000/19125 [38:35<1:02:11,  3.25it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-7000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-7000/spiece.model\n",
            "{'loss': 1.9998, 'learning_rate': 0.0006078431372549019, 'epoch': 1.18}\n",
            " 39% 7500/19125 [41:20<1:03:42,  3.04it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-7500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-7500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-7500/spiece.model\n",
            "{'loss': 1.9942, 'learning_rate': 0.0005816993464052288, 'epoch': 1.25}\n",
            " 42% 8000/19125 [44:06<54:40,  3.39it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-8000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-8000/spiece.model\n",
            "{'loss': 1.9828, 'learning_rate': 0.0005555555555555556, 'epoch': 1.33}\n",
            " 44% 8500/19125 [46:50<58:27,  3.03it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-8500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-8500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-8500/spiece.model\n",
            "{'loss': 2.0141, 'learning_rate': 0.0005294117647058824, 'epoch': 1.41}\n",
            " 47% 9000/19125 [49:37<53:31,  3.15it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-9000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-9000/spiece.model\n",
            "{'loss': 1.9936, 'learning_rate': 0.0005032679738562092, 'epoch': 1.49}\n",
            " 50% 9500/19125 [52:22<52:11,  3.07it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-9500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-9500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-9500/spiece.model\n",
            "{'loss': 1.9824, 'learning_rate': 0.00047712418300653597, 'epoch': 1.57}\n",
            " 52% 10000/19125 [55:08<45:01,  3.38it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-10000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-10000/spiece.model\n",
            "{'loss': 1.9924, 'learning_rate': 0.0004509803921568628, 'epoch': 1.65}\n",
            " 55% 10500/19125 [57:53<43:37,  3.30it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-10500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-10500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-10500/spiece.model\n",
            "{'loss': 1.9744, 'learning_rate': 0.00042483660130718953, 'epoch': 1.73}\n",
            " 58% 11000/19125 [1:00:38<44:29,  3.04it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-11000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-11000/spiece.model\n",
            "{'loss': 1.9821, 'learning_rate': 0.0003986928104575164, 'epoch': 1.8}\n",
            " 60% 11500/19125 [1:03:23<38:31,  3.30it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-11500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-11500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-11500/spiece.model\n",
            "{'loss': 1.9648, 'learning_rate': 0.00037254901960784314, 'epoch': 1.88}\n",
            " 63% 12000/19125 [1:06:06<38:23,  3.09it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-12000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-12000/spiece.model\n",
            "{'loss': 1.967, 'learning_rate': 0.00034640522875816995, 'epoch': 1.96}\n",
            " 65% 12500/19125 [1:08:52<33:20,  3.31it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-12500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-12500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-12500/spiece.model\n",
            "{'loss': 1.8858, 'learning_rate': 0.0003202614379084967, 'epoch': 2.04}\n",
            " 68% 13000/19125 [1:11:36<32:44,  3.12it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-13000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-13000/spiece.model\n",
            "{'loss': 1.7908, 'learning_rate': 0.00029411764705882356, 'epoch': 2.12}\n",
            " 71% 13500/19125 [1:14:21<30:49,  3.04it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-13500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-13500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-13500/spiece.model\n",
            "{'loss': 1.8026, 'learning_rate': 0.0002679738562091503, 'epoch': 2.2}\n",
            " 73% 14000/19125 [1:17:08<27:04,  3.15it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-14000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-14000/spiece.model\n",
            "{'loss': 1.8018, 'learning_rate': 0.00024183006535947714, 'epoch': 2.27}\n",
            " 76% 14500/19125 [1:19:53<23:21,  3.30it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-14500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-14500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-14500/spiece.model\n",
            "{'loss': 1.8144, 'learning_rate': 0.00021568627450980395, 'epoch': 2.35}\n",
            " 78% 15000/19125 [1:22:40<20:33,  3.35it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-15000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-15000/spiece.model\n",
            "{'loss': 1.7762, 'learning_rate': 0.00018954248366013073, 'epoch': 2.43}\n",
            " 81% 15500/19125 [1:25:26<19:54,  3.03it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-15500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-15500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-15500/spiece.model\n",
            "{'loss': 1.7786, 'learning_rate': 0.00016339869281045753, 'epoch': 2.51}\n",
            " 84% 16000/19125 [1:28:12<16:47,  3.10it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-16000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-16000/spiece.model\n",
            "{'loss': 1.8039, 'learning_rate': 0.00013725490196078434, 'epoch': 2.59}\n",
            " 86% 16500/19125 [1:30:58<14:02,  3.12it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-16500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-16500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-16500/spiece.model\n",
            "{'loss': 1.8044, 'learning_rate': 0.0001111111111111111, 'epoch': 2.67}\n",
            " 89% 17000/19125 [1:33:43<11:31,  3.07it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-17000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-17000/spiece.model\n",
            "{'loss': 1.779, 'learning_rate': 8.496732026143791e-05, 'epoch': 2.75}\n",
            " 92% 17500/19125 [1:36:30<08:29,  3.19it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-17500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-17500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-17500/spiece.model\n",
            "{'loss': 1.7686, 'learning_rate': 5.882352941176471e-05, 'epoch': 2.82}\n",
            " 94% 18000/19125 [1:39:16<06:10,  3.04it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-18000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-18000/spiece.model\n",
            "{'loss': 1.7857, 'learning_rate': 3.2679738562091506e-05, 'epoch': 2.9}\n",
            " 97% 18500/19125 [1:42:03<03:22,  3.09it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-18500\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-18500/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-18500/spiece.model\n",
            "{'loss': 1.7901, 'learning_rate': 6.535947712418301e-06, 'epoch': 2.98}\n",
            " 99% 19000/19125 [1:44:51<00:39,  3.16it/s]Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2/checkpoint-19000\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-19000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/checkpoint-19000/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/checkpoint-19000/spiece.model\n",
            "100% 19125/19125 [1:45:36<00:00,  3.01it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6336.1695, 'train_samples_per_second': 3.018, 'epoch': 3.0}\n",
            "100% 19125/19125 [1:45:36<00:00,  3.02it/s]\n",
            "Saving model checkpoint to /content/drive/MyDrive/tst-summarization-2\n",
            "Configuration saved in /content/drive/MyDrive/tst-summarization-2/config.json\n",
            "Model weights saved in /content/drive/MyDrive/tst-summarization-2/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/tst-summarization-2/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/tst-summarization-2/special_tokens_map.json\n",
            "Copy vocab file to /content/drive/MyDrive/tst-summarization-2/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =        3.0\n",
            "  init_mem_cpu_alloc_delta   =     1751MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =      230MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =      -21MB\n",
            "  train_mem_cpu_peaked_delta =      139MB\n",
            "  train_mem_gpu_alloc_delta  =      695MB\n",
            "  train_mem_gpu_peaked_delta =     4328MB\n",
            "  train_runtime              = 1:45:36.16\n",
            "  train_samples              =      25500\n",
            "  train_samples_per_second   =      3.018\n",
            "05/16/2021 18:26:29 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13368\n",
            "  Batch size = 4\n",
            "100% 3342/3342 [2:04:10<00:00,  2.23s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_gen_len              =    59.5408\n",
            "  eval_loss                 =     1.9886\n",
            "  eval_mem_cpu_alloc_delta  =       35MB\n",
            "  eval_mem_cpu_peaked_delta =       26MB\n",
            "  eval_mem_gpu_alloc_delta  =        0MB\n",
            "  eval_mem_gpu_peaked_delta =      474MB\n",
            "  eval_rouge1               =    37.4668\n",
            "  eval_rouge2               =    15.8604\n",
            "  eval_rougeL               =    26.4583\n",
            "  eval_rougeLsum            =    34.9684\n",
            "  eval_runtime              = 2:04:11.51\n",
            "  eval_samples              =      13368\n",
            "  eval_samples_per_second   =      1.794\n",
            "05/16/2021 20:30:41 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 11490\n",
            "  Batch size = 4\n",
            "100% 2873/2873 [1:23:58<00:00,  1.75s/it]***** predict metrics *****\n",
            "  predict_gen_len            =    59.1717\n",
            "  predict_loss               =     1.9699\n",
            "  predict_rouge1             =    37.2387\n",
            "  predict_rouge2             =    15.7194\n",
            "  predict_rougeL             =    26.4306\n",
            "  predict_rougeLsum          =    34.7499\n",
            "  predict_runtime            = 1:47:07.73\n",
            "  predict_samples            =      11490\n",
            "  predict_samples_per_second =      1.788\n",
            "  test_mem_cpu_alloc_delta   =       51MB\n",
            "  test_mem_cpu_peaked_delta  =       19MB\n",
            "  test_mem_gpu_alloc_delta   =        0MB\n",
            "  test_mem_gpu_peaked_delta  =      470MB\n",
            "100% 2873/2873 [1:57:01<00:00,  2.44s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bacf3sxVn_fD"
      },
      "source": [
        "  !python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path \"Helsinki-NLP/opus-mt-ru-en\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --dataset_name cnn_dailymail \\\n",
        "    --dataset_config \"3.0.0\" \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /tmp/tst-summarization \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S34YpQrZI8Wi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027e5df4-4570-48fb-febf-aad4adcf72b1"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization \\\n",
        "    --do_predict \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin.csv \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /content/drive/MyDrive/t5-small-automin \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 12:02:58.180961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/19/2021 12:02:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/19/2021 12:02:59 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/t5-small-automin', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May19_12-02-59_55d905815f0c', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/t5-small-automin', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/19/2021 12:02:59 - WARNING - datasets.builder -   Using custom data configuration default-8f3b29795c3b552d\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-8f3b29795c3b552d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8f3b29795c3b552d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:03<00:00,  3.26s/ba]\n",
            "05/19/2021 12:03:08 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 18\n",
            "  Batch size = 8\n",
            "100% 3/3 [00:04<00:00,  1.49s/it]***** predict metrics *****\n",
            "  init_mem_cpu_alloc_delta   =     1749MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =      230MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  predict_gen_len            =    50.9444\n",
            "  predict_loss               =     4.7228\n",
            "  predict_rouge1             =     8.9708\n",
            "  predict_rouge2             =     0.4333\n",
            "  predict_rougeL             =     6.5599\n",
            "  predict_rougeLsum          =     8.2797\n",
            "  predict_runtime            = 0:00:08.27\n",
            "  predict_samples            =         18\n",
            "  predict_samples_per_second =      2.177\n",
            "  test_mem_cpu_alloc_delta   =       19MB\n",
            "  test_mem_cpu_peaked_delta  =        0MB\n",
            "  test_mem_gpu_alloc_delta   =        0MB\n",
            "  test_mem_gpu_peaked_delta  =      735MB\n",
            "100% 3/3 [00:07<00:00,  2.63s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hTMtBZxc992",
        "outputId": "531ce2cb-d9ae-47f7-9978-5bc536218d44"
      },
      "source": [
        "!python examples/pytorch/summarization/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/tst-summarization-2 \\\n",
        "    --do_predict \\\n",
        "    --validation_file /content/drive/MyDrive/automin_processed/val_automin.csv \\\n",
        "    --test_file /content/drive/MyDrive/automin_processed/test_automin.csv \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /content/drive/MyDrive/t5-small-automin \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-19 12:57:25.021262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/19/2021 12:57:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/19/2021 12:57:26 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/t5-small-automin', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/May19_12-57-26_55d905815f0c', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/t5-small-automin', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "05/19/2021 12:57:26 - WARNING - datasets.builder -   Using custom data configuration default-8f3b29795c3b552d\n",
            "05/19/2021 12:57:26 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-8f3b29795c3b552d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-2/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/tst-summarization-2/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32100\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-2/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/tst-summarization-2/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/tst-summarization-2/spiece.model\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/tst-summarization-2/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/tst-summarization-2/tokenizer_config.json\n",
            "loading file None\n",
            "loading weights file /content/drive/MyDrive/tst-summarization-2/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/tst-summarization-2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:05<00:00,  5.97s/ba]\n",
            "05/19/2021 12:57:49 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 18\n",
            "  Batch size = 8\n",
            "100% 3/3 [00:03<00:00,  1.28s/it]***** predict metrics *****\n",
            "  init_mem_cpu_alloc_delta   =     1745MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =      230MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  predict_gen_len            =    54.3889\n",
            "  predict_loss               =     4.5394\n",
            "  predict_rouge1             =     9.1346\n",
            "  predict_rouge2             =     0.7785\n",
            "  predict_rougeL             =     6.6988\n",
            "  predict_rougeLsum          =     7.7323\n",
            "  predict_runtime            = 0:00:09.52\n",
            "  predict_samples            =         18\n",
            "  predict_samples_per_second =       1.89\n",
            "  test_mem_cpu_alloc_delta   =       19MB\n",
            "  test_mem_cpu_peaked_delta  =        0MB\n",
            "  test_mem_gpu_alloc_delta   =        0MB\n",
            "  test_mem_gpu_peaked_delta  =      735MB\n",
            "100% 3/3 [00:08<00:00,  2.69s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78DjHAS2r41F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}